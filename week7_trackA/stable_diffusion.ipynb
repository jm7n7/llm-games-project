{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e16be43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jam/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/jam/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/jam/Library/Python/3.9/lib/python/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.\n",
      "  deprecate(\"Transformer2DModelOutput\", \"1.0.0\", deprecation_message)\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import warnings\n",
    "import diffusers\n",
    "import accelerate\n",
    "import transformers\n",
    "\n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline\n",
    "#pip install torch==2.3.1 diffusers==0.29.0 transformers==4.41.2 accelerate==0.30.1\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dddb2b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Diagnosing Environment ---\n",
      "Python Executable: /Library/Developer/CommandLineTools/usr/bin/python3\n",
      "PyTorch version: 2.3.1\n",
      "Diffusers version: 0.29.0\n",
      "Transformers version: 4.41.2\n",
      "Accelerate version: 0.30.1\n",
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Verify the environment from within the script ---\n",
    "print(\"--- Diagnosing Environment ---\")\n",
    "print(f\"Python Executable: {sys.executable}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Diffusers version: {diffusers.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"Accelerate version: {accelerate.__version__}\")\n",
    "print(\"----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1cc67e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check if a GPU is available and set the device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3d331a",
   "metadata": {},
   "source": [
    "# Track A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cebd9eb",
   "metadata": {},
   "source": [
    "## Load the Stable Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c36b371c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00, 11.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Model loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2: Attempt to load the model with a basic configuration ---\n",
    "pipe = diffusers.StableDiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    ")\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "print(\"\\n✅ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f37c73",
   "metadata": {},
   "source": [
    "### Generate 3 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c611df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "OUTPUT_DIR = \"output_images\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40752962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating image for prompt: 'a two dimensional renaisaunce era crown in a pop-art graphic style'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [03:38<00:00,  4.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image as output_images/output_1.png\n",
      "Generating image for prompt: 'a destroyed medievil castle bombarded by artillery'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [04:01<00:00,  4.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image as output_images/output_2.png\n",
      "Generating image for prompt: 'a wooden chess board'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [04:03<00:00,  4.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image as output_images/output_3.png\n",
      "Image generation complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 3: Generate an image ---\n",
    "prompts = [\n",
    "\"a two dimensional renaisaunce era crown in a pop-art graphic style\",\n",
    "\"a destroyed medievil castle bombarded by artillery\",\n",
    "\"a wooden chess board\"\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(f\"Generating image for prompt: '{prompt}'\")\n",
    "    \n",
    "    # Generate the image\n",
    "    image = pipe(prompt).images[0]\n",
    "    \n",
    "    # Save the image to the output directory\n",
    "    output_path = os.path.join(OUTPUT_DIR, f\"output_{i+1}.png\")\n",
    "    image.save(output_path)\n",
    "    print(f\"Saved image as {output_path}\")\n",
    "\n",
    "print(\"Image generation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5780fb6",
   "metadata": {},
   "source": [
    "## Apply One Edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ece07bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating edited image...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [03:42<00:00,  4.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image as output_images/edited_output_3.png\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    init_image = Image.open(\"output_images/output_3.png\").convert(\"RGB\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'test_output.png' not found. Please generate it first.\")\n",
    "    exit()\n",
    "\n",
    "# Resize for consistency\n",
    "init_image = init_image.resize((768, 512))\n",
    "init_image.show()\n",
    "\n",
    "\n",
    "# --- 3. Define the Edit ---\n",
    "prompt = \"The chess board should have a grid of alternating light and dark squares\"\n",
    "\n",
    "# --- 4. Generate the Edited Image ---\n",
    "print(\"Generating edited image...\")\n",
    "# 'strength' controls how much the new image differs from the original (0.0 to 1.0)\n",
    "# A higher strength allows for more creative changes.\n",
    "edited_image = pipe(prompt=prompt, image=init_image, strength=0.10).images[0]\n",
    "edited_image.show()\n",
    "\n",
    "# --- 5. Save the Result ---\n",
    "output_path = os.path.join(OUTPUT_DIR, f\"edited_output_3.png\")\n",
    "edited_image.save(output_path)\n",
    "print(f\"Saved image as {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cee3d1f",
   "metadata": {},
   "source": [
    "## Train LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0493cf1",
   "metadata": {},
   "source": [
    "I am currently going to skip over this section of the assignment. Having to clone the full repository to do the LoRA training is not something that will advance our project along forward. It is a nice excersize in learning about image generation model training, but for the scope of this project and the decreasing timeline, I will be skipping this part. I have the code below, but will not be utilizing it further.\n",
    "\n",
    "https://github.com/huggingface/diffusers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61b4c5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡️ After training, place your LoRA weights in: /Users/jam/Documents/git/Capstone/llm-games-project/TrackA/lora_weights\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "DATA_DIR = Path(\"lora_data\")      # put 10–20 images of your SAFE concept here\n",
    "LORA_OUT = Path(\"lora_weights\")   # will contain your trained LoRA\n",
    "LORA_OUT.mkdir(exist_ok=True, parents=True)\n",
    "MODEL_ID = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "# Example CLI command (uncomment and adapt):\n",
    "# !accelerate launch \\\n",
    "#   diffusers/examples/text_to_image/train_text_to_image_lora.py \\\n",
    "#   --pretrained_model_name_or_path={MODEL_ID} \\\n",
    "#   --instance_data_dir={DATA_DIR} \\\n",
    "#   --output_dir={LORA_OUT} \\\n",
    "#   --train_batch_size=1 --gradient_accumulation_steps=4 \\\n",
    "#   --learning_rate=1e-4 --lr_warmup_steps=0 \\\n",
    "#   --max_train_steps=1000 --mixed_precision=\"fp16\"\n",
    "\n",
    "print(\"➡️ After training, place your LoRA weights in:\", LORA_OUT.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "123970f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of loading LoRA:\n",
    "# pipe.load_lora_weights(str(LORA_OUT))\n",
    "# lora_prompts = [\n",
    "#     \"your concept in a modern flat illustration, teal accents\",\n",
    "#     \"your concept in a photorealistic lab interior, soft light\"\n",
    "# ]\n",
    "# for i,p in enumerate(lora_prompts):\n",
    "#     img = pipe(prompt=p, num_inference_steps=25, guidance_scale=7.5).images[0]\n",
    "\n",
    "#     output_path = os.path.join(OUTPUT_DIR, f\"lora_out_{i}.png\")\n",
    "#     img.save(output_path)\n",
    "#     print(f\"Saved image as {output_path}\")\n",
    "\n",
    "# print(\"Done. Exported images are in:\", os.path.abspath(OUTPUT_DIR))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
